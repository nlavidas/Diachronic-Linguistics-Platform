#!/usr/bin/env python3
"""
24/7 AUTONOMOUS DIACHRONIC AI AGENT
- Runs continuously collecting & processing historical retranslations
- 30-minute daily consultation with user
- Integrates open source translation & lightweight ML tools
"""

import os
import sys
import time
import json
import sqlite3
import requests
import schedule
import logging
from datetime import datetime, timedelta
from collections import defaultdict
import subprocess
import threading
import queue

# Open source tools imports
try:
    import argostranslate.package
    import argostranslate.translate
except:
    print("Installing Argos Translate...")
    subprocess.run([sys.executable, "-m", "pip", "install", "argostranslate"])

try:
    import fasttext
except:
    print("Installing fastText...")
    subprocess.run([sys.executable, "-m", "pip", "install", "fasttext"])

try:
    from sentence_transformers import SentenceTransformer
except:
    print("Installing sentence-transformers...")
    subprocess.run([sys.executable, "-m", "pip", "install", "sentence-transformers"])

try:
    from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification
except:
    print("Installing transformers...")
    subprocess.run([sys.executable, "-m", "pip", "install", "transformers"])

# Fix encoding
if sys.platform == 'win32':
    sys.stdout.reconfigure(encoding='utf-8')

# Logging setup
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler('agent_24_7.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)

class DiachronicAutonomousAgent:
    """
    24/7 Agent that collects historical retranslations,
    uses open source tools, and has daily consultations
    """
    
    def __init__(self):
        self.start_time = datetime.now()
        self.consultation_queue = queue.Queue()
        self.daily_findings = []
        self.issues_to_discuss = []
        
        # Open source translation tools
        self.translation_tools = {
            'argos': None,  # Argos Translate
            'opus_mt': None,  # Helsinki NLP models
            'nllb': None,  # Meta's NLLB
            'libre': None  # LibreTranslate
        }
        
        # Lightweight ML tools
        self.ml_tools = {
            'fasttext': None,  # Language identification & embeddings
            'sentence_bert': None,  # Sentence embeddings
            'pos_tagger': None,  # POS tagging
            'ner': None  # Named entity recognition
        }
        
        # Historical text sources
        self.sources = {
            'retranslations': {
                'homer': {
                    'ancient': ['greek_original'],
                    'medieval': ['byzantine_greek', 'arabic_translation'],
                    'renaissance': ['chapman_1611', 'hobbes_1676'],
                    'enlightenment': ['pope_1720', 'cowper_1791'],
                    'victorian': ['derby_1865', 'lang_1883', 'butler_1898'],
                    'modern': ['lattimore_1951', 'fitzgerald_1961', 'fagles_1990']
                },
                'bible': {
                    'ancient': ['septuagint', 'vulgate'],
                    'medieval': ['wycliffe_1382'],
                    'reformation': ['luther_1522', 'tyndale_1526'],
                    'early_modern': ['geneva_1560', 'kjv_1611'],
                    'modern': ['rsv_1952', 'niv_1978', 'nrsv_1989']
                },
                'metamorphoses': {
                    'ancient': ['ovid_original'],
                    'medieval': ['french_ovide'],
                    'renaissance': ['golding_1567'],
                    'modern': ['humphries_1955', 'mandelbaum_1993']
                }
            }
        }
        
        self.init_workspace()
        self.init_ml_tools()
        self.init_translation_tools()
        
    def init_workspace(self):
        """Initialize complete workspace"""
        dirs = [
            "daily_reports",
            "consultation_logs",
            "translations/automated",
            "translations/historical",
            "ml_models",
            "embeddings",
            "alignments",
            "analysis/diachronic",
            "analysis/valency",
            "tools/configs"
        ]
        
        for d in dirs:
            os.makedirs(d, exist_ok=True)
            
        # Initialize database
        self.db = sqlite3.connect('diachronic_24_7.db', check_same_thread=False)
        self.init_database()
        
    def init_database(self):
        """Initialize comprehensive database"""
        cursor = self.db.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS daily_activities (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                activity_type TEXT,
                details TEXT,
                success BOOLEAN,
                findings TEXT
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS retranslations (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                work TEXT,
                translator TEXT,
                year INTEGER,
                period TEXT,
                language TEXT,
                source_url TEXT,
                file_path TEXT,
                processed BOOLEAN DEFAULT 0,
                embeddings_created BOOLEAN DEFAULT 0,
                aligned BOOLEAN DEFAULT 0
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS consultation_topics (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                date DATE,
                topic TEXT,
                priority INTEGER,
                discussed BOOLEAN DEFAULT 0,
                user_feedback TEXT,
                action_taken TEXT
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS ml_experiments (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                tool TEXT,
                experiment TEXT,
                results TEXT,
                performance_metrics TEXT
            )
        ''')
        
        self.db.commit()
        
    def init_ml_tools(self):
        """Initialize lightweight ML tools"""
        logging.info("ü§ñ Initializing ML tools...")
        
        try:
            # FastText for language identification
            logging.info("  Loading FastText...")
            # Download pretrained model if not exists
            if not os.path.exists('ml_models/lid.176.bin'):
                logging.info("  Downloading FastText language identification model...")
                url = "https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin"
                r = requests.get(url)
                with open('ml_models/lid.176.bin', 'wb') as f:
                    f.write(r.content)
            
            self.ml_tools['fasttext'] = fasttext.load_model('ml_models/lid.176.bin')
            
            # Sentence transformers for embeddings
            logging.info("  Loading sentence transformers...")
            self.ml_tools['sentence_bert'] = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
            
            # POS tagger
            logging.info("  Loading POS tagger...")
            self.ml_tools['pos_tagger'] = pipeline(
                "token-classification",
                model="wietsedv/bert-pos-cased"
            )
            
            logging.info("‚úÖ ML tools initialized")
            
        except Exception as e:
            logging.error(f"Error initializing ML tools: {e}")
            
    def init_translation_tools(self):
        """Initialize open source translation tools"""
        logging.info("üåê Initializing translation tools...")
        
        try:
            # Argos Translate
            logging.info("  Setting up Argos Translate...")
            argostranslate.package.update_package_index()
            available_packages = argostranslate.package.get_available_packages()
            
            # Install Greek to English if not installed
            greek_en = [p for p in available_packages 
                       if p.from_code == 'el' and p.to_code == 'en']
            if greek_en and not argostranslate.package.get_installed_packages():
                argostranslate.package.install_from_path(greek_en[0].download())
            
            self.translation_tools['argos'] = True
            
            # Setup for other tools (LibreTranslate, OPUS-MT)
            logging.info("‚úÖ Translation tools ready")
            
        except Exception as e:
            logging.error(f"Error with translation tools: {e}")
            
    def run_24_7(self):
        """Main 24/7 execution loop"""
        logging.info("üåü Starting 24/7 Autonomous Diachronic Agent")
        
        # Schedule daily consultation
        schedule.every().day.at("10:00").do(self.daily_consultation)
        
        # Schedule various tasks throughout the day
        schedule.every(2).hours.do(self.collect_retranslations)
        schedule.every(3).hours.do(self.process_with_ml)
        schedule.every(4).hours.do(self.align_translations)
        schedule.every(6).hours.do(self.analyze_diachronic_patterns)
        schedule.every().day.at("02:00").do(self.deep_night_processing)
        schedule.every().day.at("06:00").do(self.generate_daily_report)
        
        # Start continuous processing in separate thread
        processing_thread = threading.Thread(target=self.continuous_processing)
        processing_thread.daemon = True
        processing_thread.start()
        
        # Main loop
        while True:
            schedule.run_pending()
            time.sleep(60)  # Check every minute
            
    def continuous_processing(self):
        """Continuous background processing"""
        while True:
            try:
                # Process queue items
                if not self.consultation_queue.empty():
                    item = self.consultation_queue.get()
                    self.process_queue_item(item)
                
                # Check for new texts every 30 minutes
                time.sleep(1800)
                self.check_new_sources()
                
            except Exception as e:
                logging.error(f"Error in continuous processing: {e}")
                
    def collect_retranslations(self):
        """Collect historical retranslations using various methods"""
        logging.info("üìö Collecting retranslations...")
        
        findings = []
        
        # 1. Project Gutenberg API
        try:
            # Search for multiple translations
            searches = [
                "Homer Iliad translation",
                "Bible translation English",
                "Metamorphoses Ovid translation",
                "Plutarch Lives translation"
            ]
            
            for search in searches:
                results = self.search_gutenberg(search)
                findings.extend(results)
                
        except Exception as e:
            logging.error(f"Gutenberg search error: {e}")
            
        # 2. Internet Archive
        try:
            ia_results = self.search_internet_archive()
            findings.extend(ia_results)
        except Exception as e:
            logging.error(f"Internet Archive error: {e}")
            
        # 3. Process findings with ML
        for finding in findings:
            self.process_finding_with_ml(finding)
            
        # Log activity
        self.log_activity('collect_retranslations', f"Found {len(findings)} texts", True)
        
    def process_finding_with_ml(self, finding):
        """Process a found text with ML tools"""
        try:
            text_sample = finding.get('text_sample', '')
            
            if text_sample and self.ml_tools['fasttext']:
                # Language identification
                lang_pred = self.ml_tools['fasttext'].predict(text_sample.replace('\n', ' '))
                language = lang_pred[0][0].replace('__label__', '')
                confidence = lang_pred[1][0]
                
                finding['detected_language'] = language
                finding['lang_confidence'] = confidence
                
                # Create embeddings
                if self.ml_tools['sentence_bert']:
                    sentences = text_sample.split('.')[:5]  # First 5 sentences
                    embeddings = self.ml_tools['sentence_bert'].encode(sentences)
                    
                    # Save embeddings
                    embed_path = f"embeddings/{finding['id']}_embeddings.npy"
                    import numpy as np
                    np.save(embed_path, embeddings)
                    finding['embeddings_path'] = embed_path
                    
                # POS tagging sample
                if self.ml_tools['pos_tagger'] and language == 'en':
                    pos_results = self.ml_tools['pos_tagger'](text_sample[:500])
                    finding['pos_sample'] = pos_results
                    
        except Exception as e:
            logging.error(f"ML processing error: {e}")
            
    def align_translations(self):
        """Align parallel translations using ML"""
        logging.info("üîó Aligning translations...")
        
        # Get unaligned parallel texts
        cursor = self.db.cursor()
        cursor.execute("""
            SELECT id, work, file_path 
            FROM retranslations 
            WHERE aligned = 0 AND processed = 1
            GROUP BY work
            HAVING COUNT(*) > 1
        """)
        
        works = defaultdict(list)
        for id, work, path in cursor.fetchall():
            works[work].append((id, path))
            
        for work, texts in works.items():
            if len(texts) >= 2:
                self.align_work_translations(work, texts)
                
    def align_work_translations(self, work, text_pairs):
        """Align multiple translations of the same work"""
        logging.info(f"  Aligning {len(text_pairs)} translations of {work}")
        
        try:
            # Use sentence embeddings for alignment
            if self.ml_tools['sentence_bert']:
                all_embeddings = []
                
                for text_id, path in text_pairs:
                    if os.path.exists(path):
                        with open(path, 'r', encoding='utf-8') as f:
                            content = f.read()
                            
                        # Split into sentences
                        sentences = self.split_sentences(content)[:100]  # First 100
                        
                        # Create embeddings
                        embeddings = self.ml_tools['sentence_bert'].encode(sentences)
                        all_embeddings.append({
                            'id': text_id,
                            'sentences': sentences,
                            'embeddings': embeddings
                        })
                        
                # Align using cosine similarity
                self.align_by_similarity(all_embeddings, work)
                
        except Exception as e:
            logging.error(f"Alignment error for {work}: {e}")
            
    def analyze_diachronic_patterns(self):
        """Analyze patterns across time periods"""
        logging.info("üìä Analyzing diachronic patterns...")
        
        # Analyze translation strategies over time
        patterns = {
            'lexical_choices': self.analyze_lexical_evolution(),
            'syntactic_patterns': self.analyze_syntactic_changes(),
            'stylistic_features': self.analyze_style_evolution()
        }
        
        # Add findings to discussion queue
        if patterns['lexical_choices']:
            self.issues_to_discuss.append({
                'topic': 'Lexical Evolution Patterns',
                'data': patterns['lexical_choices'],
                'priority': 2
            })
            
    def deep_night_processing(self):
        """Heavy processing during night hours"""
        logging.info("üåô Starting deep night processing...")
        
        # 1. Download large texts
        self.download_large_corpora()
        
        # 2. Train custom models
        self.train_custom_models()
        
        # 3. Generate comprehensive alignments
        self.generate_comprehensive_alignments()
        
    def daily_consultation(self):
        """30-minute daily consultation with user"""
        logging.info("\n" + "="*60)
        logging.info("üó£Ô∏è DAILY CONSULTATION SESSION STARTING")
        logging.info("="*60)
        
        print("\nüí¨ Good morning! This is your daily 30-minute consultation.")
        print("Let me share what I've discovered and discuss any issues.\n")
        
        # 1. Present daily findings
        print("üìä DAILY FINDINGS:")
        print("-" * 40)
        
        cursor = self.db.cursor()
        cursor.execute("""
            SELECT activity_type, details, findings 
            FROM daily_activities 
            WHERE date(timestamp) = date('now')
            AND success = 1
            ORDER BY timestamp DESC
            LIMIT 10
        """)
        
        for activity, details, findings in cursor.fetchall():
            print(f"‚Ä¢ {activity}: {details}")
            if findings:
                print(f"  ‚Üí {findings}")
                
        # 2. Discuss issues
        print("\n‚ùì ISSUES TO DISCUSS:")
        print("-" * 40)
        
        for issue in self.issues_to_discuss[:5]:  # Top 5 issues
            print(f"\n{issue['topic']}:")
            print(f"Priority: {'‚≠ê' * issue.get('priority', 1)}")
            print(f"Details: {issue.get('data', 'No details')}")
            
            # Get user input
            response = self.get_user_input(f"Your thoughts on this? (or 'skip'): ")
            
            if response.lower() != 'skip':
                self.process_user_feedback(issue['topic'], response)
                
        # 3. Get user requests
        print("\nüéØ WHAT WOULD YOU LIKE ME TO FOCUS ON TODAY?")
        print("-" * 40)
        
        focus = self.get_user_input("Enter focus areas (or 'continue' for autonomous): ")
        
        if focus.lower() != 'continue':
            self.add_user_priorities(focus)
            
        # 4. Quick decisions
        print("\n‚ö° QUICK DECISIONS NEEDED:")
        print("-" * 40)
        
        decisions = [
            "Should I prioritize Greek texts over Latin today?",
            "Download more Bible translations or focus on Homer?",
            "Run deeper ML analysis or collect more texts?"
        ]
        
        for decision in decisions:
            response = self.get_user_input(f"{decision} (y/n/skip): ")
            if response.lower() == 'y':
                self.process_decision(decision, True)
            elif response.lower() == 'n':
                self.process_decision(decision, False)
                
        # 5. End consultation
        print("\n‚úÖ Consultation complete! Returning to autonomous mode.")
        print("Next consultation: tomorrow at 10:00 AM")
        print("="*60 + "\n")
        
        # Log consultation
        self.log_consultation()
        
    def get_user_input(self, prompt, timeout=300):
        """Get user input with timeout (5 minutes)"""
        import select
        import sys
        
        print(prompt, end='', flush=True)
        
        # Simple input for Windows
        if sys.platform == 'win32':
            try:
                import msvcrt
                import time
                
                start_time = time.time()
                input_chars = []
                
                while True:
                    if msvcrt.kbhit():
                        char = msvcrt.getche()
                        if char in [b'\r', b'\n']:
                            print()
                            return ''.join(input_chars)
                        input_chars.append(char.decode('utf-8', errors='ignore'))
                    
                    if time.time() - start_time > timeout:
                        print("\n[Timeout - continuing autonomously]")
                        return "skip"
                        
                    time.sleep(0.1)
            except:
                # Fallback to regular input
                try:
                    return input()
                except:
                    return "skip"
        else:
            # Unix-like systems
            ready = select.select([sys.stdin], [], [], timeout)
            if ready[0]:
                return sys.stdin.readline().strip()
            else:
                print("\n[Timeout - continuing autonomously]")
                return "skip"
                
    def process_user_feedback(self, topic, feedback):
        """Process and store user feedback"""
        cursor = self.db.cursor()
        cursor.execute("""
            INSERT INTO consultation_topics 
            (date, topic, priority, discussed, user_feedback)
            VALUES (date('now'), ?, 1, 1, ?)
        """, (topic, feedback))
        self.db.commit()
        
        # Adjust agent behavior based on feedback
        if "more" in feedback.lower():
            self.increase_priority(topic)
        elif "less" in feedback.lower():
            self.decrease_priority(topic)
            
    def add_user_priorities(self, focus_areas):
        """Add user-specified priorities"""
        areas = focus_areas.split(',')
        for area in areas:
            area = area.strip()
            if area:
                self.consultation_queue.put({
                    'type': 'user_priority',
                    'task': area,
                    'timestamp': datetime.now()
                })
                
    def log_activity(self, activity_type, details, success, findings=None):
        """Log agent activities"""
        cursor = self.db.cursor()
        cursor.execute("""
            INSERT INTO daily_activities 
            (activity_type, details, success, findings)
            VALUES (?, ?, ?, ?)
        """, (activity_type, details, success, json.dumps(findings) if findings else None))
        self.db.commit()
        
    def generate_daily_report(self):
        """Generate comprehensive daily report"""
        logging.info("üìÑ Generating daily report...")
        
        report = f"""
# DIACHRONIC AGENT DAILY REPORT
Date: {datetime.now().strftime('%Y-%m-%d')}

## Activities Summary
"""
        
        cursor = self.db.cursor()
        
        # Activity counts
        cursor.execute("""
            SELECT activity_type, COUNT(*), SUM(success)
            FROM daily_activities
            WHERE date(timestamp) = date('now')
            GROUP BY activity_type
        """)
        
        for activity, total, successful in cursor.fetchall():
            report += f"- {activity}: {successful}/{total} successful\n"
            
        # New retranslations found
        cursor.execute("""
            SELECT COUNT(*) FROM retranslations
            WHERE date(id) = date('now')
        """)
        new_texts = cursor.fetchone()[0]
        report += f"\n## New Texts Found: {new_texts}\n"
        
        # ML experiments
        cursor.execute("""
            SELECT tool, experiment, results
            FROM ml_experiments
            WHERE date(timestamp) = date('now')
        """)
        
        report += "\n## ML Experiments:\n"
        for tool, exp, results in cursor.fetchall():
            report += f"- {tool}: {exp}\n"
            
        # Save report
        report_path = f"daily_reports/report_{datetime.now().strftime('%Y%m%d')}.md"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)
            
        self.daily_findings.append(report_path)
        
    def search_gutenberg(self, query):
        """Search Project Gutenberg"""
        # Implementation for Gutenberg search
        results = []
        # Would implement actual search here
        return results
        
    def search_internet_archive(self):
        """Search Internet Archive for historical texts"""
        # Implementation for IA search
        results = []
        # Would implement actual search here
        return results
        
    def split_sentences(self, text):
        """Split text into sentences"""
        # Simple sentence splitter
        import re
        sentences = re.split(r'[.!?]+', text)
        return [s.strip() for s in sentences if s.strip()]
        
    def train_custom_models(self):
        """Train custom models during night"""
        logging.info("üß† Training custom models...")
        
        # Train fastText embeddings on collected texts
        # Train custom translation models
        # Fine-tune existing models
        
    def log_consultation(self):
        """Log consultation session"""
        log_path = f"consultation_logs/consultation_{datetime.now().strftime('%Y%m%d')}.json"
        
        consultation_data = {
            'date': datetime.now().isoformat(),
            'issues_discussed': len(self.issues_to_discuss),
            'user_feedback_received': True,
            'new_priorities': self.consultation_queue.qsize()
        }
        
        with open(log_path, 'w') as f:
            json.dump(consultation_data, f, indent=2)
            
    def analyze_lexical_evolution(self):
        """Analyze how word choices change over time"""
        # Implementation
        return {}
        
    def analyze_syntactic_changes(self):
        """Analyze syntactic pattern changes"""
        # Implementation
        return {}
        
    def analyze_style_evolution(self):
        """Analyze stylistic evolution"""
        # Implementation
        return {}
        
    def increase_priority(self, topic):
        """Increase priority for a topic"""
        # Adjust scheduling
        pass
        
    def decrease_priority(self, topic):
        """Decrease priority for a topic"""
        # Adjust scheduling
        pass
        
    def process_decision(self, decision, choice):
        """Process user decisions from consultation"""
        # Implement decision logic
        pass
        
    def download_large_corpora(self):
        """Download large corpora during night"""
        # Implementation
        pass
        
    def generate_comprehensive_alignments(self):
        """Generate comprehensive alignments"""
        # Implementation
        pass
        
    def align_by_similarity(self, embeddings_data, work):
        """Align texts using embedding similarity"""
        # Implementation
        pass
        
    def check_new_sources(self):
        """Check for new sources periodically"""
        # Implementation
        pass
        
    def process_queue_item(self, item):
        """Process items from consultation queue"""
        # Implementation
        pass


if __name__ == "__main__":
    agent = DiachronicAutonomousAgent()
    
    print("üöÄ Diachronic Autonomous Agent Starting...")
    print("üìÖ Daily consultations at 10:00 AM")
    print("ü§ñ Using open source ML tools")
    print("üåê Using open source translation tools")
    print("\nPress Ctrl+C to stop\n")
    
    try:
        agent.run_24_7()
    except KeyboardInterrupt:
        print("\nüëã Agent stopped by user")